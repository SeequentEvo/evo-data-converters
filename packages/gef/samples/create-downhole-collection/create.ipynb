{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook for testing downhole collection construction\n",
    "\n",
    "This notebook is for testing the creation of a DownholeCollection (intermediary object) and conversion to an Evo Downhole Collection.\n",
    "\n",
    "A cell at the end of this notebook is available to publish the result if desired.\n",
    "\n",
    "This notebook's main purpose is testing each part of the GEF conversion and adding in an IntervalTable as part of the final payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from evo.notebooks import ServiceManagerWidget\n",
    "\n",
    "# Credentials can be provided from .env or filled into second params below.\n",
    "# Use `uv run --env-file .env` to load environment variables from .env file.\n",
    "client_id = os.getenv(\"EVO_CLIENT_ID\", \"\")\n",
    "base_uri = os.getenv(\"EVO_BASE_URI\", \"\")\n",
    "discovery_url = os.getenv(\"EVO_DISCOVERY_URL\", \"\")\n",
    "\n",
    "manager = await ServiceManagerWidget.with_auth_code(\n",
    "    client_id=client_id, base_uri=base_uri, discovery_url=discovery_url\n",
    ").login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.data_converters.common import create_evo_object_service_and_data_client\n",
    "\n",
    "object_service_client, data_client = create_evo_object_service_and_data_client(service_manager_widget=manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "from pprint import pp\n",
    "from evo.data_converters.gef.converter import create_from_parsed_gef_cpts, parse_gef_files\n",
    "from evo.data_converters.common.objects import DownholeCollectionToGeoscienceObject\n",
    "from evo.data_converters.common.objects.downhole_collection import ColumnMapping\n",
    "\n",
    "# Data files in data/input have been synthentically generated from a python script.\n",
    "gef_files = glob.glob(\"data/input/*.gef\")\n",
    "parsed_cpt_files = parse_gef_files(gef_files)\n",
    "# pprint(parsed_cpt_files)\n",
    "\n",
    "downhole_collection = create_from_parsed_gef_cpts(parsed_cpt_files)\n",
    "\n",
    "lithologies: list[str] = [\"sandstone\", \"limestone\", \"shale\", \"granite\", \"basalt\", \"mudstone\", \"conglomerate\"]\n",
    "\n",
    "# Generate a somewhat complex interval table\n",
    "interval_table_data = {\n",
    "    # Hole index will come from LOCA_ID and mapped to the index of the hole_id in the collars df using 1-based arrays\n",
    "    \"hole_index\": [],\n",
    "    \"GEOL_TOP\": [],\n",
    "    \"GEOL_BASE\": [],\n",
    "    \"GEOL_DENSITY\": [],\n",
    "    \"GEOL_LITHOLOGY\": [],\n",
    "}\n",
    "\n",
    "# each hole has intervals from 0 to 20m (based on sample data in ./data/input)\n",
    "for hole_id in range(1, 5):\n",
    "    cur_depth = 0\n",
    "\n",
    "    while cur_depth < 20:\n",
    "        interval_size = random.uniform(1.5, 4.0)\n",
    "\n",
    "        # Make sure we don't exceed 20m\n",
    "        if cur_depth + interval_size > 20:\n",
    "            interval_size = 20 - cur_depth\n",
    "\n",
    "        density = random.uniform(1.2, 2.6)\n",
    "\n",
    "        interval_table_data[\"hole_index\"].append(hole_id)\n",
    "        interval_table_data[\"GEOL_TOP\"].append(cur_depth)\n",
    "        interval_table_data[\"GEOL_BASE\"].append(cur_depth + interval_size)\n",
    "        interval_table_data[\"GEOL_DENSITY\"].append(density)\n",
    "        interval_table_data[\"GEOL_LITHOLOGY\"].append(random.choice(lithologies))\n",
    "\n",
    "        cur_depth += interval_size\n",
    "\n",
    "interval_table = pd.DataFrame(interval_table_data)\n",
    "\n",
    "# create categorical attribute\n",
    "interval_table[\"GEOL_LITHOLOGY\"] = interval_table[\"GEOL_LITHOLOGY\"].astype(\"category\")\n",
    "\n",
    "display(interval_table)\n",
    "\n",
    "downhole_collection.add_measurement_table(\n",
    "    input=interval_table, column_mapping=ColumnMapping(FROM_COLUMNS=[\"GEOL_TOP\"], TO_COLUMNS=[\"GEOL_BASE\"])\n",
    ")\n",
    "\n",
    "converter = DownholeCollectionToGeoscienceObject(dhc=downhole_collection, data_client=data_client)\n",
    "geoscience_object = converter.convert()\n",
    "\n",
    "pp(geoscience_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Optionally, you can publish the constructed downhole collection by running the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.data_converters.common import publish_geoscience_objects\n",
    "\n",
    "result = publish_geoscience_objects(\n",
    "    object_models=[geoscience_object],\n",
    "    object_service_client=object_service_client,\n",
    "    data_client=data_client,\n",
    "    path_prefix=\"gef-notebook\",\n",
    "    overwrite_existing_objects=True,\n",
    ")\n",
    "\n",
    "pp(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
